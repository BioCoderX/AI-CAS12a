{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7854d370-9c0c-4b7e-a5d1-b02b9db99ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ XGBoost available, version: 3.0.1\n",
      "=== XGBoost Time Series Binary Classification Detailed Analysis ===\n",
      "\n",
      "1. Loading data...\n",
      "Trying to read file using openpyxl engine...\n",
      "✓ Successfully read data using openpyxl\n",
      "Successfully read data, shape: (375, 7)\n",
      "Column names: ['ID', 'X1', 'X2', 'X3', 'X4', 'X5', 'label']\n",
      "\n",
      "Data preview:\n",
      "   ID       X1       X2       X3       X4       X5  label\n",
      "0   1   910.23  2174.87  3399.19  4568.75  5669.20      1\n",
      "1   2   840.10  2221.61  3554.34  4828.21  6014.50      1\n",
      "2   3   811.46  2028.56  3192.89  4306.80  5343.23      1\n",
      "3   4  2413.83  5045.10  6452.97  7616.88  8631.15      1\n",
      "4   5  2442.67  5111.87  6512.67  7653.70  8638.73      1\n",
      "\n",
      "Final data dimensions: X=(375, 5), y=(375,)\n",
      "Label distribution: Positive=263 (70.1%), Negative=112 (29.9%)\n",
      "Data dimensions: X=(375, 5), y=(375,)\n",
      "Label distribution: Positive=263 (70.1%), Negative=112 (29.9%)\n",
      "\n",
      "2. Data preprocessing and splitting...\n",
      "Training set size: 225\n",
      "Validation set size: 75\n",
      "Test set size: 75\n",
      "Training set positive ratio: 0.702\n",
      "Validation set positive ratio: 0.707\n",
      "Test set positive ratio: 0.693\n",
      "\n",
      "3. Creating and training XGBoost model...\n",
      "Starting training XGBoost Detailed Analysis Model...\n",
      "Training samples: 225, Validation samples: 75\n",
      "Training error: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "✓ Completed using basic training method\n",
      "\n",
      "Validation performance:\n",
      "Accuracy: 0.9867\n",
      "AUC: 0.9996\n",
      "F1-score: 0.9905\n",
      "\n",
      "Feature importance:\n",
      "X4 (4-minute): 0.4205\n",
      "X5 (5-minute): 0.3815\n",
      "X3 (3-minute): 0.1420\n",
      "X2 (2-minute): 0.0336\n",
      "X1 (1-minute): 0.0225\n",
      "\n",
      "4. Performing cross validation...\n",
      "\n",
      "Performing 5-fold cross validation...\n",
      "Cross-validation AUC: 0.9970 (+/- 0.0120)\n",
      "\n",
      "5. Final test set evaluation...\n",
      "\n",
      "=== Final Test Results ===\n",
      "Test Accuracy: 1.0000\n",
      "Test AUC: 1.0000\n",
      "Test F1-Score: 1.0000\n",
      "Cross-validation AUC Mean: 0.9970 (±0.0060)\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00        23\n",
      "    Positive       1.00      1.00      1.00        52\n",
      "\n",
      "    accuracy                           1.00        75\n",
      "   macro avg       1.00      1.00      1.00        75\n",
      "weighted avg       1.00      1.00      1.00        75\n",
      "\n",
      "\n",
      "6. Generating detailed visualization analysis...\n",
      "Plotting comprehensive analysis charts...\n",
      "Saving individual plots to xgboost_plots/ directory...\n",
      "✓ Saved: 02_feature_importance.svg\n",
      "✓ Saved: 03_roc_curves.svg\n",
      "✓ Saved: 04_precision_recall_curves.svg\n",
      "✓ Saved: 05_confusion_matrix.svg\n",
      "✓ Saved: 06_probability_distribution.svg\n",
      "✓ Saved: 07_performance_radar.svg\n",
      "✓ Saved: 08_learning_curve.svg\n",
      "✓ Saved: 09_feature_correlation.svg\n",
      "✓ Saved: 10_tree_or_importance_pie.svg\n",
      "Plotting training process analysis...\n",
      "No training history data available for visualization\n",
      "Plotting feature interaction analysis...\n",
      "✓ Saved: 15_feature_importance_ranking.svg\n",
      "✓ Saved: 16_feature_distribution.svg\n",
      "✓ Saved: 17_correlation_network.svg\n",
      "✓ Saved: 18_cumulative_importance.svg\n",
      "Creating performance report table...\n",
      "✓ Saved: 19_performance_report_table.svg\n",
      "\n",
      "7. Generating additional validation plots...\n",
      "Plotting actual vs predicted values...\n",
      "✓ Saved: 20_actual_vs_predicted_comprehensive.svg\n",
      "✓ Saved: 21_actual_vs_predicted_combined.svg\n",
      "Creating accuracy metrics comparison...\n",
      "✓ Saved: 22_accuracy_metrics_comparison.svg\n",
      "\n",
      "Metrics Comparison Summary:\n",
      "      Dataset  Accuracy  Precision    Recall  F1-Score       AUC        AP  \\\n",
      "0    Training  0.995556        1.0  0.993671  0.996825  1.000000  1.000000   \n",
      "1  Validation  0.986667        1.0  0.981132  0.990476  0.999571  0.999651   \n",
      "2        Test  1.000000        1.0  1.000000  1.000000  1.000000  1.000000   \n",
      "\n",
      "        MAE  \n",
      "0  0.017294  \n",
      "1  0.026202  \n",
      "2  0.012533  \n",
      "Performing residual analysis...\n",
      "✓ Saved: 23_residual_analysis.svg\n",
      "Performing mae_learning_curve...\n",
      "\n",
      "Generating MAE Learning Curve...\n",
      "✓ Saved: 24_mae_learning_curve.svg\n",
      "✓ Saved: 25_mae_learning_curve_detailed.svg\n",
      "\n",
      "=== MAE Learning Curve Summary ===\n",
      "Initial Training MAE (10% data): 0.1264\n",
      "Final Training MAE (100% data): 0.0188\n",
      "Initial Validation MAE: 0.1697\n",
      "Final Validation MAE: 0.0328\n",
      "MAE Improvement (Validation): 0.1368\n",
      "Final MAE Gap: 0.0141\n",
      "Assessment: Good generalization\n",
      "\n",
      "=== XGBoost Model Analysis Summary ===\n",
      "✓ Model achieved AUC of 1.000 on test set, performance is Excellent\n",
      "✓ Cross-validation results are stable, AUC standard deviation is 0.006\n",
      "✓ Most important feature is X4 (4-minute), importance: 0.420\n",
      "\n",
      "Feature Importance Ranking:\n",
      "1. X4 (4-minute): 0.420\n",
      "2. X5 (5-minute): 0.381\n",
      "3. X3 (3-minute): 0.142\n",
      "4. X2 (2-minute): 0.034\n",
      "5. X1 (1-minute): 0.022\n",
      "\n",
      "Recommend using this XGBoost model for time series binary classification prediction\n",
      "\n",
      "8. Validating Real-Data.xlsx...\n",
      "\n",
      "=== Validating Real-Data.xlsx ===\n",
      "Trying to read file using openpyxl engine...\n",
      "✓ Successfully read data using openpyxl\n",
      "Successfully read data, shape: (18, 7)\n",
      "Column names: ['ID', 'X1', 'X2', 'X3', 'X4', 'X5', 'label']\n",
      "\n",
      "Data preview:\n",
      "   ID        X1        X2        X3        X4        X5  label\n",
      "0  Y1   8188.91   9090.09   9996.08  10912.47  11842.49      1\n",
      "1  Y2   8587.42   9986.83  11381.78  12782.20  14190.27      1\n",
      "2  Y3  10196.73  11917.06  13630.46  15345.08  17063.00      1\n",
      "3  Y4  10794.90  11837.48  12894.13  13966.94  15060.23      1\n",
      "4  Y5  11492.27  12719.03  13932.07  15140.83  16344.38      1\n",
      "\n",
      "Final data dimensions: X=(18, 5), y=(18,)\n",
      "Label distribution: Positive=15 (83.3%), Negative=3 (16.7%)\n",
      "Real-Data dimensions: X=(18, 5), y=(18,)\n",
      "Label distribution: Positive=15 (83.3%), Negative=3 (16.7%)\n",
      "\n",
      "=== Real-Data Evaluation Results ===\n",
      "Accuracy: 1.0000\n",
      "AUC: 1.0000\n",
      "F1-score: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         3\n",
      "    Positive       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n",
      "\n",
      "Generating Real-Data visualizations in real_data_plots/ directory...\n",
      "✓ Saved: real_data_roc_curve.svg\n",
      "✓ Saved: real_data_pr_curve.svg\n",
      "✓ Saved: real_data_confusion_matrix.svg\n",
      "✓ Saved: real_data_probability_distribution.svg\n",
      "✓ Saved: real_data_performance_radar.svg\n",
      "✓ Saved: real_data_feature_importance.svg\n",
      "✓ Saved: real_data_feature_distribution.svg\n",
      "✓ Saved: real_data_performance_report.svg\n",
      "✓ Saved: real_data_prediction_error.svg\n",
      "✓ Saved: real_data_feature_correlation.svg\n",
      "\n",
      "=== Real-Data Validation Complete ===\n",
      "All plots have been saved in 'real_data_plots' directory\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                           average_precision_score, f1_score, precision_score, recall_score)\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XGBoost import\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"✓ XGBoost available, version:\", xgb.__version__)\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"✗ XGBoost not installed, please run: pip install xgboost\")\n",
    "    exit()\n",
    "\n",
    "# Set English font support\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Generate mock dataset (375 samples, 5 time features)\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generate simulated time series data\"\"\"\n",
    "    n_samples = 375\n",
    "    \n",
    "    # Generate basic time features\n",
    "    X1 = np.random.normal(10, 2, n_samples)  # 1-minute feature\n",
    "    X2 = X1 + np.random.normal(0, 1, n_samples)  # 2-minute feature (correlated with X1)\n",
    "    X3 = X2 + np.random.normal(0, 1.5, n_samples)  # 3-minute feature\n",
    "    X4 = X3 + np.random.normal(0, 1, n_samples)  # 4-minute feature\n",
    "    X5 = X4 + np.random.normal(0, 2, n_samples)  # 5-minute feature\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = np.column_stack([X1, X2, X3, X4, X5])\n",
    "    \n",
    "    # Generate labels (based on feature combination)\n",
    "    trend = (X5 - X1) / 4  # Calculate trend\n",
    "    mean_value = np.mean(X, axis=1)  # Mean value\n",
    "    \n",
    "    # Generate labels based on trend and mean value\n",
    "    prob_positive = 1 / (1 + np.exp(-(0.3 * trend + 0.1 * mean_value - 2)))\n",
    "    y = np.random.binomial(1, prob_positive, n_samples)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def load_excel_data(file_path='DATA-5.xlsx', sheet_name=0):\n",
    "    \"\"\"Load data from Excel file\"\"\"\n",
    "    try:\n",
    "        # Try different engines to read Excel file\n",
    "        engines = ['openpyxl', 'xlrd']\n",
    "        df = None\n",
    "        \n",
    "        for engine in engines:\n",
    "            try:\n",
    "                print(f\"Trying to read file using {engine} engine...\")\n",
    "                df = pd.read_excel(file_path, sheet_name=sheet_name, engine=engine)\n",
    "                print(f\"✓ Successfully read data using {engine}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"✗ {engine} engine failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if df is None:\n",
    "            raise Exception(\"All engines failed to read the file\")\n",
    "            \n",
    "        print(f\"Successfully read data, shape: {df.shape}\")\n",
    "        print(f\"Column names: {list(df.columns)}\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(\"\\nData preview:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_values = df.isnull().sum()\n",
    "        if missing_values.any():\n",
    "            print(\"\\nMissing values statistics:\")\n",
    "            print(missing_values[missing_values > 0])\n",
    "            print(\"Removing rows with missing values...\")\n",
    "            df = df.dropna()\n",
    "            print(f\"Shape after removal: {df.shape}\")\n",
    "        \n",
    "        # Automatically identify feature columns and label column\n",
    "        feature_cols = []\n",
    "        label_col = None\n",
    "        \n",
    "        # Look for X1-X5 feature columns (case insensitive)\n",
    "        for col in df.columns:\n",
    "            col_str = str(col).strip().upper()\n",
    "            if col_str in ['X1', 'X2', 'X3', 'X4', 'X5']:\n",
    "                feature_cols.append(col)\n",
    "        \n",
    "        # Sort by X1,X2,X3,X4,X5 order\n",
    "        feature_order = {'X1': 1, 'X2': 2, 'X3': 3, 'X4': 4, 'X5': 5}\n",
    "        feature_cols.sort(key=lambda x: feature_order.get(str(x).strip().upper(), 999))\n",
    "        \n",
    "        # Look for label column\n",
    "        for col in df.columns:\n",
    "            col_str = str(col).strip().lower()\n",
    "            if col_str in ['label', 'y', 'target', 'class', 'lable']:\n",
    "                label_col = col\n",
    "                break\n",
    "        \n",
    "        # If standard column names not found, smart guess\n",
    "        if len(feature_cols) != 5:\n",
    "            print(f\"\\nOnly found {len(feature_cols)} standard feature columns: {feature_cols}\")\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if len(numeric_cols) >= 5:\n",
    "                feature_cols = numeric_cols[:5]\n",
    "                print(f\"Auto-selecting first 5 numeric columns as features: {feature_cols}\")\n",
    "            else:\n",
    "                return None, None\n",
    "        \n",
    "        if label_col is None:\n",
    "            remaining_cols = [col for col in df.columns if col not in feature_cols]\n",
    "            if remaining_cols:\n",
    "                label_col = remaining_cols[0]\n",
    "                print(f\"Auto-selecting column as label: {label_col}\")\n",
    "            else:\n",
    "                return None, None\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X = df[feature_cols].values\n",
    "        y = df[label_col].values\n",
    "        \n",
    "        # Process labels\n",
    "        unique_labels = np.unique(y)\n",
    "        if set(unique_labels) == {0, 1} or set(unique_labels) == {0.0, 1.0}:\n",
    "            y = y.astype(int)\n",
    "        elif len(unique_labels) == 2:\n",
    "            min_val, max_val = min(unique_labels), max(unique_labels)\n",
    "            y = (y == max_val).astype(int)\n",
    "        else:\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"\\nFinal data dimensions: X={X.shape}, y={y.shape}\")\n",
    "        print(f\"Label distribution: Positive={np.sum(y)} ({100*np.mean(y):.1f}%), Negative={len(y)-np.sum(y)} ({100*(1-np.mean(y)):.1f}%)\")\n",
    "        \n",
    "        return X, y\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found {file_path}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def prepare_data(X, y):\n",
    "    \"\"\"Data standardization and split into train/validation/test sets\"\"\"\n",
    "    # Ensure input is numpy array format\n",
    "    if hasattr(X, 'values'):\n",
    "        X = X.values\n",
    "    if hasattr(y, 'values'):\n",
    "        y = y.values\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # First split into train and temp sets (test + validation)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Then split temp set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Training set size: {X_train_scaled.shape[0]}\")\n",
    "    print(f\"Validation set size: {X_val_scaled.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test_scaled.shape[0]}\")\n",
    "    print(f\"Training set positive ratio: {np.mean(y_train):.3f}\")\n",
    "    print(f\"Validation set positive ratio: {np.mean(y_val):.3f}\")\n",
    "    print(f\"Test set positive ratio: {np.mean(y_test):.3f}\")\n",
    "    \n",
    "    return (X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "            y_train, y_val, y_test, scaler)\n",
    "\n",
    "class XGBoostDetailedModel:\n",
    "    \"\"\"Enhanced XGBoost model with detailed training process monitoring and visualization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        self.name = \"XGBoost Detailed Analysis Model\"\n",
    "        self.feature_importance = None\n",
    "        self.training_history = {}\n",
    "        self.feature_names = ['X1 (1-minute)', 'X2 (2-minute)', 'X3 (3-minute)', 'X4 (4-minute)', 'X5 (5-minute)']\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train model and record detailed process\"\"\"\n",
    "        print(f\"Starting training {self.name}...\")\n",
    "        print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "        \n",
    "        # Create validation set\n",
    "        eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "        eval_names = ['train', 'validation']\n",
    "        \n",
    "        try:\n",
    "            # Train with early stopping\n",
    "            self.model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=eval_set,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose=10\n",
    "            )\n",
    "            print(\"✓ Training completed\")\n",
    "            \n",
    "            # Get training history\n",
    "            self.training_history = self.model.evals_result()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training error: {e}\")\n",
    "            # Use basic training method\n",
    "            self.model.fit(X_train, y_train)\n",
    "            print(\"✓ Completed using basic training method\")\n",
    "        \n",
    "        # Validation evaluation\n",
    "        val_pred = self.model.predict(X_val)\n",
    "        val_prob = self.model.predict_proba(X_val)[:, 1]\n",
    "        val_accuracy = accuracy_score(y_val, val_pred)\n",
    "        val_auc = roc_auc_score(y_val, val_prob)\n",
    "        val_f1 = f1_score(y_val, val_pred)\n",
    "        \n",
    "        print(f\"\\nValidation performance:\")\n",
    "        print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"AUC: {val_auc:.4f}\")\n",
    "        print(f\"F1-score: {val_f1:.4f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            self.feature_importance = self.model.feature_importances_\n",
    "            importance_dict = dict(zip(self.feature_names, self.feature_importance))\n",
    "            print(f\"\\nFeature importance:\")\n",
    "            for feature, importance in sorted(importance_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"{feature}: {importance:.4f}\")\n",
    "        \n",
    "        return val_accuracy, val_auc, val_f1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict\"\"\"\n",
    "        pred = self.model.predict(X)\n",
    "        prob = self.model.predict_proba(X)[:, 1]\n",
    "        return pred, prob\n",
    "    \n",
    "    def cross_validate(self, X, y, cv=5):\n",
    "        \"\"\"Cross validation\"\"\"\n",
    "        print(f\"\\nPerforming {cv}-fold cross validation...\")\n",
    "        cv_scores = cross_val_score(self.model, X, y, cv=cv, scoring='roc_auc')\n",
    "        print(f\"Cross-validation AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        return cv_scores\n",
    "\n",
    "def save_individual_plots(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Generate and save individual plots as SVG files\"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    import os\n",
    "    output_dir = \"xgboost_plots\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving individual plots to {output_dir}/ directory...\")\n",
    "    \n",
    "    # Predict on all datasets\n",
    "    train_pred, train_prob = model.predict(X_train)\n",
    "    val_pred, val_prob = model.predict(X_val)\n",
    "    test_pred, test_prob = model.predict(X_test)\n",
    "    \n",
    "    # 1. Training loss curve\n",
    "    if model.training_history:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        train_loss = model.training_history.get('train', {}).get('logloss', [])\n",
    "        val_loss = model.training_history.get('validation', {}).get('logloss', [])\n",
    "        \n",
    "        if train_loss and val_loss:\n",
    "            epochs = range(1, len(train_loss) + 1)\n",
    "            ax.plot(epochs, train_loss, 'b-', label='Training loss', linewidth=2)\n",
    "            ax.plot(epochs, val_loss, 'r-', label='Validation loss', linewidth=2)\n",
    "            ax.set_xlabel('Training rounds')\n",
    "            ax.set_ylabel('Logarithmic loss')\n",
    "            ax.set_title('Loss Curve During Training', fontsize=14, fontweight='bold')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Mark best iteration\n",
    "            if hasattr(model.model, 'best_iteration'):\n",
    "                best_iter = model.model.best_iteration\n",
    "                ax.axvline(x=best_iter, color='green', linestyle='--', alpha=0.7, \n",
    "                          label=f'Best round: {best_iter}')\n",
    "                ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/01_training_loss_curve.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"✓ Saved: 01_training_loss_curve.svg\")\n",
    "    \n",
    "    # 2. Feature importance plot\n",
    "    if model.feature_importance is not None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': model.feature_names,\n",
    "            'importance': model.feature_importance\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        bars = ax.barh(importance_df['feature'], importance_df['importance'], \n",
    "                      color=plt.cm.viridis(np.linspace(0, 1, len(importance_df))))\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        ax.set_title('Feature Importance Ranking', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, importance_df['importance'])):\n",
    "            ax.text(value + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{value:.3f}', va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/02_feature_importance.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"✓ Saved: 02_feature_importance.svg\")\n",
    "    \n",
    "    # 3. ROC curves comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Training set ROC\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, train_prob)\n",
    "    auc_train = roc_auc_score(y_train, train_prob)\n",
    "    \n",
    "    # Validation set ROC\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val, val_prob)\n",
    "    auc_val = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Test set ROC\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, test_prob)\n",
    "    auc_test = roc_auc_score(y_test, test_prob)\n",
    "    \n",
    "    ax.plot(fpr_train, tpr_train, 'b-', linewidth=2, label=f'Training (AUC = {auc_train:.3f})')\n",
    "    ax.plot(fpr_val, tpr_val, 'g-', linewidth=2, label=f'Validation (AUC = {auc_val:.3f})')\n",
    "    ax.plot(fpr_test, tpr_test, 'r-', linewidth=2, label=f'Test (AUC = {auc_test:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "    ax.set_xlabel('False Positive Rate (FPR)')\n",
    "    ax.set_ylabel('True Positive Rate (TPR)')\n",
    "    ax.set_title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/03_roc_curves.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 03_roc_curves.svg\")\n",
    "    \n",
    "    # 4. Precision-Recall curves\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    precision_train, recall_train, _ = precision_recall_curve(y_train, train_prob)\n",
    "    ap_train = average_precision_score(y_train, train_prob)\n",
    "    \n",
    "    precision_val, recall_val, _ = precision_recall_curve(y_val, val_prob)\n",
    "    ap_val = average_precision_score(y_val, val_prob)\n",
    "    \n",
    "    precision_test, recall_test, _ = precision_recall_curve(y_test, test_prob)\n",
    "    ap_test = average_precision_score(y_test, test_prob)\n",
    "    \n",
    "    ax.plot(recall_train, precision_train, 'b-', linewidth=2, label=f'Training (AP = {ap_train:.3f})')\n",
    "    ax.plot(recall_val, precision_val, 'g-', linewidth=2, label=f'Validation (AP = {ap_val:.3f})')\n",
    "    ax.plot(recall_test, precision_test, 'r-', linewidth=2, label=f'Test (AP = {ap_test:.3f})')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/04_precision_recall_curves.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 04_precision_recall_curves.svg\")\n",
    "    \n",
    "    # 5. Confusion matrix\n",
    "    plt.rcParams['font.size'] = 18  # 全局默认字体大小设置为10\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, test_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, annot_kws={\"fontsize\": 28},\n",
    "                xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "    ax.set_xlabel('Predicted Label',fontsize=28)\n",
    "    ax.set_ylabel('True Label',fontsize=28)\n",
    "    ax.set_title('Test Set Confusion Matrix', fontsize=28, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/05_confusion_matrix.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 05_confusion_matrix.svg\")\n",
    "    \n",
    "    # 6. Prediction probability distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    pos_probs = test_prob[y_test == 1]\n",
    "    neg_probs = test_prob[y_test == 0]\n",
    "    \n",
    "    ax.hist(neg_probs, bins=30, alpha=0.7, label='Negative', color='red', density=True)\n",
    "    ax.hist(pos_probs, bins=30, alpha=0.7, label='Positive', color='blue', density=True)\n",
    "    ax.set_xlabel('Prediction Probability')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Predicted Probability Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/06_probability_distribution.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 06_probability_distribution.svg\")\n",
    "    \n",
    "    # 7. Performance metrics radar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Calculate various performance metrics\n",
    "    metrics_names = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1-score', 'Specificity']\n",
    "    \n",
    "    accuracy_test = accuracy_score(y_test, test_pred)\n",
    "    precision_test = precision_score(y_test, test_pred)\n",
    "    recall_test = recall_score(y_test, test_pred)\n",
    "    f1_test = f1_score(y_test, test_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, test_pred).ravel()\n",
    "    specificity_test = tn / (tn + fp)\n",
    "    \n",
    "    metrics_values = [accuracy_test, auc_test, precision_test, recall_test, f1_test, specificity_test]\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the plot\n",
    "    metrics_values += metrics_values[:1]  # Close the plot\n",
    "    \n",
    "    ax.plot(angles, metrics_values, 'o-', linewidth=2, color='red')\n",
    "    ax.fill(angles, metrics_values, alpha=0.25, color='red')\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Test Set Performance Metrics Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/07_performance_radar.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 07_performance_radar.svg\")\n",
    "    \n",
    "    # 8. Learning curve\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for size in train_sizes:\n",
    "        n_samples = int(size * len(X_train))\n",
    "        if n_samples < 10:\n",
    "            continue\n",
    "        \n",
    "        # Subsample training\n",
    "        indices = np.random.choice(len(X_train), n_samples, replace=False)\n",
    "        X_sub = X_train[indices]\n",
    "        y_sub = y_train[indices]\n",
    "        \n",
    "        # Train temporary model\n",
    "        temp_model = xgb.XGBClassifier(\n",
    "            n_estimators=50, max_depth=4, learning_rate=0.1, \n",
    "            random_state=42, eval_metric='logloss', use_label_encoder=False\n",
    "        )\n",
    "        temp_model.fit(X_sub, y_sub)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_pred_temp = temp_model.predict_proba(X_sub)[:, 1]\n",
    "        val_pred_temp = temp_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        train_scores.append(roc_auc_score(y_sub, train_pred_temp))\n",
    "        val_scores.append(roc_auc_score(y_val, val_pred_temp))\n",
    "    \n",
    "    if train_scores and val_scores:\n",
    "        valid_sizes = train_sizes[:len(train_scores)]\n",
    "        ax.plot(valid_sizes * len(X_train), train_scores, 'b-', linewidth=4, label='Training AUC')\n",
    "        ax.plot(valid_sizes * len(X_train), val_scores, 'r-', linewidth=4, label='Validation AUC')\n",
    "        ax.tick_params(axis='both',which='major',labelsize=24)\n",
    "        ax.set_xlabel('Training Sample Size', fontsize=28)\n",
    "        ax.set_ylabel('AUC Score', fontsize=28)\n",
    "        ax.set_title('Learning Curve', fontsize=28, fontweight='bold')\n",
    "        ax.legend(fontsize=24)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/08_learning_curve.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 08_learning_curve.svg\")\n",
    "    \n",
    "    # 9. Feature correlation heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    feature_data = pd.DataFrame(X_train, columns=[f'X{i+1}' for i in range(X_train.shape[1])])\n",
    "    corr_matrix = feature_data.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, \n",
    "                cmap = sns.color_palette(\"vlag\", as_cmap=True), \n",
    "                norm=TwoSlopeNorm(vmin=0.9, vcenter=0.95, vmax=1),\n",
    "                center=0, ax=ax,\n",
    "                square=True, fmt='.2f',\n",
    "                linewidths=0.6,         \n",
    "                linecolor='white',\n",
    "                annot_kws={'fontsize':10, 'fontweight':'bold'},\n",
    "                cbar_kws={'shrink':0.8, 'ticks':[-1, -0.5, 0, 0.5, 1]})\n",
    "    ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/09_feature_correlation.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 09_feature_correlation.svg\")\n",
    "    \n",
    "    # 10. First decision tree visualization or feature importance pie chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    try:\n",
    "        # Try to plot the first decision tree\n",
    "        xgb.plot_tree(model.model, num_trees=0, ax=ax, rankdir='TB')\n",
    "        ax.set_title('XGBoost First Decision Tree Structure', fontsize=14, fontweight='bold')\n",
    "    except:\n",
    "        # If tree plotting fails, show feature importance pie chart\n",
    "        if model.feature_importance is not None:\n",
    "            ax.pie(model.feature_importance, labels=model.feature_names, autopct='%1.1f%%',\n",
    "                  startangle=90, colors=plt.cm.Set3(np.linspace(0, 1, len(model.feature_names))))\n",
    "            ax.set_title('Feature Importance Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/10_tree_or_importance_pie.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 10_tree_or_importance_pie.svg\")\n",
    "    \n",
    "    return {\n",
    "        'train_auc': auc_train,\n",
    "        'val_auc': auc_val,\n",
    "        'test_auc': auc_test,\n",
    "        'test_accuracy': accuracy_test,\n",
    "        'test_f1': f1_test,\n",
    "        'test_precision': precision_test,\n",
    "        'test_recall': recall_test\n",
    "    }\n",
    "\n",
    "def plot_training_progress(model):\n",
    "    \"\"\"Plot detailed training progress\"\"\"\n",
    "    if not model.training_history:\n",
    "        print(\"No training history data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    output_dir = \"xgboost_plots\"\n",
    "    \n",
    "    train_loss = model.training_history.get('train', {}).get('logloss', [])\n",
    "    val_loss = model.training_history.get('validation', {}).get('logloss', [])\n",
    "    \n",
    "    if not train_loss or not val_loss:\n",
    "        print(\"Training history data incomplete\")\n",
    "        return\n",
    "    \n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    # Training loss curve\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(epochs, train_loss, 'b-', linewidth=2, label='Training Loss')\n",
    "    ax.plot(epochs, val_loss, 'r-', linewidth=2, label='Validation Loss')\n",
    "    ax.set_xlabel('Training Rounds')\n",
    "    ax.set_ylabel('Logarithmic Loss')\n",
    "    ax.set_title('Training Process Loss Curve', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark best iteration\n",
    "    if hasattr(model.model, 'best_iteration') and model.model.best_iteration:\n",
    "        best_iter = model.model.best_iteration\n",
    "        ax.axvline(x=best_iter, color='green', linestyle='--', alpha=0.7)\n",
    "        ax.text(best_iter, min(val_loss), f'Best Round: {best_iter}', \n",
    "               rotation=90, verticalalignment='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/11_detailed_training_loss.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 11_detailed_training_loss.svg\")\n",
    "    \n",
    "    # Loss improvement trend\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(epochs[1:], np.diff(train_loss), 'b-', label='Training Loss Improvement', alpha=0.7)\n",
    "    ax.plot(epochs[1:], np.diff(val_loss), 'r-', label='Validation Loss Improvement', alpha=0.7)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Training Rounds')\n",
    "    ax.set_ylabel('Loss Improvement')\n",
    "    ax.set_title('Loss Improvement Trend', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/12_loss_improvement_trend.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 12_loss_improvement_trend.svg\")\n",
    "    \n",
    "    # Overfitting detection\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    overfitting = np.array(val_loss) - np.array(train_loss)\n",
    "    ax.plot(epochs, overfitting, 'purple', linewidth=2)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.fill_between(epochs, overfitting, 0, alpha=0.3, color='purple')\n",
    "    ax.set_xlabel('Training Rounds')\n",
    "    ax.set_ylabel('Validation Loss - Training Loss')\n",
    "    ax.set_title('Overfitting Detection (Higher Gap = More Overfitting)', fontweight='bold', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/13_overfitting_detection.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 13_overfitting_detection.svg\")\n",
    "    \n",
    "    # Training stability\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    window_size = min(10, len(train_loss) // 4)\n",
    "    if window_size > 1:\n",
    "        train_smooth = pd.Series(train_loss).rolling(window=window_size).std()\n",
    "        val_smooth = pd.Series(val_loss).rolling(window=window_size).std()\n",
    "        \n",
    "        ax.plot(epochs, train_smooth, 'b-', label='Training Loss Volatility', alpha=0.7)\n",
    "        ax.plot(epochs, val_smooth, 'r-', label='Validation Loss Volatility', alpha=0.7)\n",
    "        ax.set_xlabel('Training Rounds')\n",
    "        ax.set_ylabel('Loss Standard Deviation (Rolling Window)')\n",
    "        ax.set_title('Training Stability Analysis', fontweight='bold', fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/14_training_stability.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 14_training_stability.svg\")\n",
    "\n",
    "def analyze_feature_interactions(model, X, feature_names):\n",
    "    \"\"\"Analyze feature interactions\"\"\"\n",
    "    if model.feature_importance is None:\n",
    "        print(\"Model feature importance data not available\")\n",
    "        return\n",
    "    \n",
    "    output_dir = \"xgboost_plots\"\n",
    "    \n",
    "    # Feature importance comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    importance = model.feature_importance\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    \n",
    "    ax.bar(range(len(importance)), importance[indices], \n",
    "           color=plt.cm.viridis(np.linspace(0, 1, len(importance))))\n",
    "    ax.set_title('Feature Importance Ranking', fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('Feature Rank')\n",
    "    ax.set_ylabel('Importance Score')\n",
    "    ax.set_xticks(range(len(importance)))\n",
    "    ax.set_xticklabels([feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/15_feature_importance_ranking.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 15_feature_importance_ranking.svg\")\n",
    "    \n",
    "    # Feature distribution comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.boxplot([X[:, i] for i in range(X.shape[1])], \n",
    "               labels=[f'X{i+1}' for i in range(X.shape[1])])\n",
    "    ax.set_title('Feature Value Distribution', fontweight='bold', fontsize=14)\n",
    "    ax.set_ylabel('Feature Values')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/16_feature_distribution.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 16_feature_distribution.svg\")\n",
    "    \n",
    "    # Feature correlation network graph\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    feature_data = pd.DataFrame(X, columns=[f'X{i+1}' for i in range(X.shape[1])])\n",
    "    corr_matrix = feature_data.corr()\n",
    "    \n",
    "    # Create network graph layout\n",
    "    G_pos = {}\n",
    "    angle_step = 2 * np.pi / len(feature_names)\n",
    "    for i, name in enumerate(feature_names):\n",
    "        angle = i * angle_step\n",
    "        G_pos[name] = (np.cos(angle), np.sin(angle))\n",
    "    \n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    \n",
    "    # Draw nodes\n",
    "    for name, (x, y) in G_pos.items():\n",
    "        ax.scatter(x, y, s=1000, c='lightblue', alpha=0.7)\n",
    "        ax.text(x, y, name.split('(')[0], ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # Draw connection lines (correlations)\n",
    "    for i, name1 in enumerate(feature_names):\n",
    "        for j, name2 in enumerate(feature_names):\n",
    "            if i < j:  # Avoid duplicate drawing\n",
    "                corr = abs(corr_matrix.iloc[i, j])\n",
    "                if corr > 0.3:  # Only show strong correlations\n",
    "                    x1, y1 = G_pos[name1]\n",
    "                    x2, y2 = G_pos[name2]\n",
    "                    ax.plot([x1, x2], [y1, y2], 'r-', alpha=corr, linewidth=corr*3)\n",
    "    \n",
    "    ax.set_title('Feature Correlation Network (Line thickness shows correlation strength)', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/17_correlation_network.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 17_correlation_network.svg\")\n",
    "    \n",
    "    # Cumulative importance contribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    cumsum_importance = np.cumsum(sorted(importance, reverse=True))\n",
    "    cumsum_importance = cumsum_importance / cumsum_importance[-1] * 100\n",
    "    \n",
    "    ax.plot(range(1, len(cumsum_importance)+1), cumsum_importance, 'bo-', linewidth=4)\n",
    "    ax.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='80% Contribution Line',linewidth=4)\n",
    "    ax.axhline(y=95, color='orange', linestyle='--', alpha=0.7, label='95% Contribution Line',linewidth=4)\n",
    "    ax.set_xlabel('Number of Features', fontsize=28)\n",
    "    ax.tick_params(axis='both',which='major',labelsize=24)\n",
    "    ax.set_ylabel('Contribution (%)', fontsize=28) ##Cumulative Importance Contribution\n",
    "    ax.set_title('Feature Importance Cumulative Contribution', fontweight='bold', fontsize=28)\n",
    "    ax.legend( fontsize=24)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, y in enumerate(cumsum_importance):\n",
    "        if i == 0 or i == len(cumsum_importance)-1 or abs(y - 80) < 5 or abs(y - 95) < 5:\n",
    "            ax.annotate(f'{y:.1f}%', (i+1, y), textcoords=\"offset points\", \n",
    "                       xytext=(0,10), ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/18_cumulative_importance.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 18_cumulative_importance.svg\")\n",
    "\n",
    "def create_performance_report_table(y_test, test_pred, test_prob):\n",
    "    \"\"\"Create detailed performance report table\"\"\"\n",
    "    output_dir = \"xgboost_plots\"\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    accuracy_test = accuracy_score(y_test, test_pred)\n",
    "    precision_test = precision_score(y_test, test_pred)\n",
    "    recall_test = recall_score(y_test, test_pred)\n",
    "    f1_test = f1_score(y_test, test_pred)\n",
    "    auc_test = roc_auc_score(y_test, test_prob)\n",
    "    ap_test = average_precision_score(y_test, test_prob)\n",
    "    \n",
    "    # Create performance report table\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create detailed performance report\n",
    "    report = classification_report(y_test, test_pred, output_dict=True)\n",
    "    \n",
    "    report_data = [\n",
    "        ['Class', 'Precision', 'Recall', 'F1-Score', 'Support'],\n",
    "        ['Negative (0)', f\"{report['0']['precision']:.3f}\", f\"{report['0']['recall']:.3f}\", \n",
    "         f\"{report['0']['f1-score']:.3f}\", f\"{report['0']['support']:.0f}\"],\n",
    "        ['Positive (1)', f\"{report['1']['precision']:.3f}\", f\"{report['1']['recall']:.3f}\", \n",
    "         f\"{report['1']['f1-score']:.3f}\", f\"{report['1']['support']:.0f}\"],\n",
    "        ['Macro Average', f\"{report['macro avg']['precision']:.3f}\", f\"{report['macro avg']['recall']:.3f}\", \n",
    "         f\"{report['macro avg']['f1-score']:.3f}\", f\"{report['macro avg']['support']:.0f}\"],\n",
    "        ['Weighted Average', f\"{report['weighted avg']['precision']:.3f}\", f\"{report['weighted avg']['recall']:.3f}\", \n",
    "         f\"{report['weighted avg']['f1-score']:.3f}\", f\"{report['weighted avg']['support']:.0f}\"],\n",
    "        ['', '', '', '', ''],\n",
    "        ['Overall Accuracy', f\"{accuracy_test:.3f}\", '', '', f\"{len(y_test)}\"],\n",
    "        ['AUC Score', f\"{auc_test:.3f}\", '', '', ''],\n",
    "        ['Average Precision', f\"{ap_test:.3f}\", '', '', '']\n",
    "    ]\n",
    "    \n",
    "    table = ax.table(cellText=report_data[1:], colLabels=report_data[0],\n",
    "                    cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 2.0)\n",
    "    \n",
    "    # Set table style\n",
    "    for i in range(len(report_data[0])):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Highlight performance rows\n",
    "    for i in range(1, 4):  # Negative, Positive, Macro average rows\n",
    "        for j in range(len(report_data[0])):\n",
    "            if i <= 2:  # Class rows\n",
    "                table[(i, j)].set_facecolor('#E8F5E8')\n",
    "            elif i == 3:  # Macro average row\n",
    "                table[(i, j)].set_facecolor('#FFF3CD')\n",
    "    \n",
    "    ax.set_title('XGBoost Model Detailed Performance Report', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/19_performance_report_table.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 19_performance_report_table.svg\")\n",
    "\n",
    "def validate_real_data(model, scaler, real_data_path='Real-Data.xlsx'):\n",
    "    \"\"\"Validate Real-Data.xlsx using trained model\"\"\"\n",
    "    print(\"\\n=== Validating Real-Data.xlsx ===\")\n",
    "    \n",
    "    # Load real data\n",
    "    X_real, y_real = load_excel_data(real_data_path)\n",
    "    if X_real is None or y_real is None:\n",
    "        print(\"Error loading Real-Data.xlsx. Using simulated data instead.\")\n",
    "        X_real, y_real = generate_sample_data()\n",
    "    \n",
    "    print(f\"Real-Data dimensions: X={X_real.shape}, y={y_real.shape}\")\n",
    "    print(f\"Label distribution: Positive={np.sum(y_real)} ({100*np.mean(y_real):.1f}%), Negative={len(y_real)-np.sum(y_real)} ({100*(1-np.mean(y_real)):.1f}%)\")\n",
    "    \n",
    "    # Standardize using the same scaler from training\n",
    "    X_real_scaled = scaler.transform(X_real)\n",
    "    \n",
    "    # Predict\n",
    "    real_pred, real_prob = model.predict(X_real_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    real_acc = accuracy_score(y_real, real_pred)\n",
    "    real_auc = roc_auc_score(y_real, real_prob)\n",
    "    real_f1 = f1_score(y_real, real_pred)\n",
    "    real_precision = precision_score(y_real, real_pred)\n",
    "    real_recall = recall_score(y_real, real_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== Real-Data Evaluation Results ===\")\n",
    "    print(f\"Accuracy: {real_acc:.4f}\")\n",
    "    print(f\"AUC: {real_auc:.4f}\")\n",
    "    print(f\"F1-score: {real_f1:.4f}\")\n",
    "    print(f\"Precision: {real_precision:.4f}\")\n",
    "    print(f\"Recall: {real_recall:.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_real, real_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"real_data_plots\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate comprehensive visualizations\n",
    "    generate_real_data_visualizations(model, X_real_scaled, y_real, real_pred, real_prob, output_dir)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': real_acc,\n",
    "        'auc': real_auc,\n",
    "        'f1': real_f1,\n",
    "        'precision': real_precision,\n",
    "        'recall': real_recall\n",
    "    }\n",
    "\n",
    "def generate_real_data_visualizations(model, X_real, y_real, real_pred, real_prob, output_dir):\n",
    "    \"\"\"Generate comprehensive visualizations for real data\"\"\"\n",
    "    print(f\"\\nGenerating Real-Data visualizations in {output_dir}/ directory...\")\n",
    "    \n",
    "    # 1. ROC curve\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    fpr, tpr, _ = roc_curve(y_real, real_prob)\n",
    "    auc_score = roc_auc_score(y_real, real_prob)\n",
    "    \n",
    "    ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'Real-Data (AUC = {auc_score:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "    ax.set_xlabel('False Positive Rate (FPR)')\n",
    "    ax.set_ylabel('True Positive Rate (TPR)')\n",
    "    ax.set_title('ROC Curve for Real-Data', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_roc_curve.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_roc_curve.svg\")\n",
    "    \n",
    "    # 2. Precision-Recall curve\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    precision, recall, _ = precision_recall_curve(y_real, real_prob)\n",
    "    ap_score = average_precision_score(y_real, real_prob)\n",
    "    \n",
    "    ax.plot(recall, precision, 'r-', linewidth=2, label=f'Real-Data (AP = {ap_score:.3f})')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision-Recall Curve for Real-Data', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_pr_curve.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_pr_curve.svg\")\n",
    "    \n",
    "    # 3. Confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_real, real_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_title('Real-Data Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_confusion_matrix.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_confusion_matrix.svg\")\n",
    "    \n",
    "    # 4. Prediction probability distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    pos_probs = real_prob[y_real == 1]\n",
    "    neg_probs = real_prob[y_real == 0]\n",
    "    \n",
    "    ax.hist(neg_probs, bins=30, alpha=0.7, label='Negative', color='red', density=True)\n",
    "    ax.hist(pos_probs, bins=30, alpha=0.7, label='Positive', color='blue', density=True)\n",
    "    ax.set_xlabel('Prediction Probability')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Real-Data Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_probability_distribution.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_probability_distribution.svg\")\n",
    "    \n",
    "    # 5. Performance metrics radar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Calculate various performance metrics\n",
    "    metrics_names = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1-score', 'Specificity']\n",
    "    \n",
    "    accuracy = accuracy_score(y_real, real_pred)\n",
    "    precision = precision_score(y_real, real_pred)\n",
    "    recall = recall_score(y_real, real_pred)\n",
    "    f1 = f1_score(y_real, real_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_real, real_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    auc = roc_auc_score(y_real, real_prob)\n",
    "    \n",
    "    metrics_values = [accuracy, auc, precision, recall, f1, specificity]\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the plot\n",
    "    metrics_values += metrics_values[:1]  # Close the plot\n",
    "    \n",
    "    ax.plot(angles, metrics_values, 'o-', linewidth=2, color='purple')\n",
    "    ax.fill(angles, metrics_values, alpha=0.25, color='purple')\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Real-Data Performance Metrics Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_performance_radar.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_performance_radar.svg\")\n",
    "    \n",
    "    # 6. Feature importance plot (from trained model)\n",
    "    if model.feature_importance is not None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': model.feature_names,\n",
    "            'importance': model.feature_importance\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        bars = ax.barh(importance_df['feature'], importance_df['importance'], \n",
    "                      color=plt.cm.viridis(np.linspace(0, 1, len(importance_df))))\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        ax.set_title('Feature Importance Ranking', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, importance_df['importance'])):\n",
    "            ax.text(value + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{value:.3f}', va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/real_data_feature_importance.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"✓ Saved: real_data_feature_importance.svg\")\n",
    "    \n",
    "    # 7. Real data feature distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    feature_data = pd.DataFrame(X_real, columns=[f'X{i+1}' for i in range(X_real.shape[1])])\n",
    "    \n",
    "    for i, col in enumerate(feature_data.columns):\n",
    "        sns.kdeplot(feature_data[col], fill=True, alpha=0.5, label=f'X{i+1}')\n",
    "    \n",
    "    ax.set_title('Real-Data Feature Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Feature Values')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_feature_distribution.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_feature_distribution.svg\")\n",
    "    \n",
    "    # 8. Classification report table\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create detailed performance report\n",
    "    report = classification_report(y_real, real_pred, output_dict=True)\n",
    "    \n",
    "    report_data = [\n",
    "        ['Class', 'Precision', 'Recall', 'F1-Score', 'Support'],\n",
    "        ['Negative (0)', f\"{report['0']['precision']:.3f}\", f\"{report['0']['recall']:.3f}\", \n",
    "         f\"{report['0']['f1-score']:.3f}\", f\"{report['0']['support']:.0f}\"],\n",
    "        ['Positive (1)', f\"{report['1']['precision']:.3f}\", f\"{report['1']['recall']:.3f}\", \n",
    "         f\"{report['1']['f1-score']:.3f}\", f\"{report['1']['support']:.0f}\"],\n",
    "        ['Macro Average', f\"{report['macro avg']['precision']:.3f}\", f\"{report['macro avg']['recall']:.3f}\", \n",
    "         f\"{report['macro avg']['f1-score']:.3f}\", f\"{report['macro avg']['support']:.0f}\"],\n",
    "        ['Weighted Average', f\"{report['weighted avg']['precision']:.3f}\", f\"{report['weighted avg']['recall']:.3f}\", \n",
    "         f\"{report['weighted avg']['f1-score']:.3f}\", f\"{report['weighted avg']['support']:.0f}\"],\n",
    "        ['', '', '', '', ''],\n",
    "        ['Overall Accuracy', f\"{accuracy:.3f}\", '', '', f\"{len(y_real)}\"],\n",
    "        ['AUC Score', f\"{auc:.3f}\", '', '', ''],\n",
    "        ['Average Precision', f\"{ap_score:.3f}\", '', '', '']\n",
    "    ]\n",
    "    \n",
    "    table = ax.table(cellText=report_data[1:], colLabels=report_data[0],\n",
    "                    cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 2.0)\n",
    "    \n",
    "    # Set table style\n",
    "    for i in range(len(report_data[0])):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Highlight performance rows\n",
    "    for i in range(1, 4):  # Negative, Positive, Macro average rows\n",
    "        for j in range(len(report_data[0])):\n",
    "            if i <= 2:  # Class rows\n",
    "                table[(i, j)].set_facecolor('#E8F5E8')\n",
    "            elif i == 3:  # Macro average row\n",
    "                table[(i, j)].set_facecolor('#FFF3CD')\n",
    "    \n",
    "    ax.set_title('Real-Data Detailed Performance Report', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_performance_report.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_performance_report.svg\")\n",
    "    \n",
    "    # 9. Prediction error analysis\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    error = real_pred != y_real\n",
    "    correct = real_pred == y_real\n",
    "    \n",
    "    # Plot correct and incorrect predictions\n",
    "    ax.scatter(np.where(correct)[0], real_prob[correct], \n",
    "              alpha=0.5, label='Correct Prediction', color='green',s=100)\n",
    "    ax.scatter(np.where(error)[0], real_prob[error], \n",
    "              alpha=0.8, label='Incorrect Prediction', color='red', marker='x')\n",
    "    \n",
    "    # Add decision boundary\n",
    "    ax.axhline(y=0.5, color='blue', linestyle='--', alpha=0.7, label='Decision Boundary (0.5)', linewidth=4)\n",
    "    \n",
    "    ax.set_xlabel('Sample Index',fontsize=28)\n",
    "    ax.set_ylabel('Prediction Probability',fontsize=28)\n",
    "    ax.set_title('Real-Data Prediction Analysis', fontsize=28, fontweight='bold')\n",
    "    ax.tick_params(axis='both',which='major',labelsize=24)\n",
    "    ax.legend(framealpha=True, fontsize=24)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_prediction_error.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_prediction_error.svg\")\n",
    "    \n",
    "    # 10. Feature correlation heatmap for real data\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    corr_matrix = feature_data.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, \n",
    "                cmap=sns.diverging_palette(220, 20, as_cmap=True),\n",
    "                center=0, ax=ax,\n",
    "                square=True, fmt='.2f',\n",
    "                linewidths=0.6,\n",
    "                linecolor='white',\n",
    "                annot_kws={'fontsize':10, 'fontweight':'bold'},\n",
    "                cbar_kws={'shrink':0.8})\n",
    "    ax.set_title('Real-Data Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/real_data_feature_correlation.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: real_data_feature_correlation.svg\")\n",
    "\n",
    "# [Previous functions remain the same: generate_sample_data, load_excel_data, prepare_data, XGBoostDetailedModel class]\n",
    "# ... (keeping all existing functions)\n",
    "\n",
    "def plot_actual_vs_predicted(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plot actual values versus predicted values for training, validation, and test sets\n",
    "    This validates the accuracy of machine learning predictions\n",
    "    \"\"\"\n",
    "    output_dir = \"xgboost_plots\"\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    train_pred, train_prob = model.predict(X_train)\n",
    "    val_pred, val_prob = model.predict(X_val)\n",
    "    test_pred, test_prob = model.predict(X_test)\n",
    "    \n",
    "    # Create comprehensive figure with subplots\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # 1. Training Set: Actual vs Predicted (Scatter)\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    ax1.scatter(y_train, train_prob, alpha=0.6, s=50, c='blue', edgecolors='black', linewidth=0.5)\n",
    "    ax1.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    ax1.axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='Decision Boundary')\n",
    "    ax1.set_xlabel('Actual Values', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Predicted Probabilities', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(f'Training Set (n={len(y_train)})\\nAUC={roc_auc_score(y_train, train_prob):.3f}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-0.1, 1.1)\n",
    "    ax1.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    # 2. Validation Set: Actual vs Predicted (Scatter)\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    ax2.scatter(y_val, val_prob, alpha=0.6, s=50, c='green', edgecolors='black', linewidth=0.5)\n",
    "    ax2.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    ax2.axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='Decision Boundary')\n",
    "    ax2.set_xlabel('Actual Values', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Predicted Probabilities', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title(f'Validation Set (n={len(y_val)})\\nAUC={roc_auc_score(y_val, val_prob):.3f}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(-0.1, 1.1)\n",
    "    ax2.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    # 3. Test Set: Actual vs Predicted (Scatter)\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    ax3.scatter(y_test, test_prob, alpha=0.6, s=50, c='red', edgecolors='black', linewidth=0.5)\n",
    "    ax3.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    ax3.axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='Decision Boundary')\n",
    "    ax3.set_xlabel('Actual Values', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Predicted Probabilities', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title(f'Test Set (n={len(y_test)})\\nAUC={roc_auc_score(y_test, test_prob):.3f}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xlim(-0.1, 1.1)\n",
    "    ax3.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    # 4. Prediction Error Distribution - Training\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    train_errors = train_prob - y_train\n",
    "    ax4.hist(train_errors, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax4.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax4.axvline(x=np.mean(train_errors), color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Mean Error: {np.mean(train_errors):.3f}')\n",
    "    ax4.set_xlabel('Prediction Error', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title(f'Training Error Distribution\\nMAE={np.mean(np.abs(train_errors)):.3f}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Prediction Error Distribution - Validation\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    val_errors = val_prob - y_val\n",
    "    ax5.hist(val_errors, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax5.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax5.axvline(x=np.mean(val_errors), color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Mean Error: {np.mean(val_errors):.3f}')\n",
    "    ax5.set_xlabel('Prediction Error', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title(f'Validation Error Distribution\\nMAE={np.mean(np.abs(val_errors)):.3f}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Prediction Error Distribution - Test\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    test_errors = test_prob - y_test\n",
    "    ax6.hist(test_errors, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "    ax6.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax6.axvline(x=np.mean(test_errors), color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Mean Error: {np.mean(test_errors):.3f}')\n",
    "    ax6.set_xlabel('Prediction Error', fontsize=12, fontweight='bold')\n",
    "    ax6.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax6.set_title(f'Test Error Distribution\\nMAE={np.mean(np.abs(test_errors)):.3f}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Actual vs Predicted Values Analysis: Validation of ML Predictions', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/20_actual_vs_predicted_comprehensive.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 20_actual_vs_predicted_comprehensive.svg\")\n",
    "    \n",
    "    # Additional plot: Combined scatter plot with all datasets\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Add jitter to separate overlapping points\n",
    "    jitter_train = np.random.normal(0, 0.02, len(y_train))\n",
    "    jitter_val = np.random.normal(0, 0.02, len(y_val))\n",
    "    jitter_test = np.random.normal(0, 0.02, len(y_test))\n",
    "    \n",
    "    ax.scatter(y_train + jitter_train, train_prob, alpha=0.5, s=60, c='blue', \n",
    "               label=f'Training (n={len(y_train)}, AUC={roc_auc_score(y_train, train_prob):.3f})',\n",
    "               edgecolors='black', linewidth=0.5)\n",
    "    ax.scatter(y_val + jitter_val, val_prob, alpha=0.5, s=60, c='green', \n",
    "               label=f'Validation (n={len(y_val)}, AUC={roc_auc_score(y_val, val_prob):.3f})',\n",
    "               edgecolors='black', linewidth=0.5)\n",
    "    ax.scatter(y_test + jitter_test, test_prob, alpha=0.5, s=60, c='red', \n",
    "               label=f'Test (n={len(y_test)}, AUC={roc_auc_score(y_test, test_prob):.3f})',\n",
    "               edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=3, label='Perfect Prediction', alpha=0.7)\n",
    "    ax.axhline(y=0.5, color='purple', linestyle='--', linewidth=2, alpha=0.5, label='Decision Boundary')\n",
    "    \n",
    "    ax.set_xlabel('Actual Values', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Predicted Probabilities', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Combined Actual vs Predicted: All Datasets', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/21_actual_vs_predicted_combined.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 21_actual_vs_predicted_combined.svg\")\n",
    "\n",
    "\n",
    "def plot_prediction_accuracy_metrics(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Create detailed accuracy metrics comparison table and visualization\n",
    "    \"\"\"\n",
    "    output_dir = \"xgboost_plots\"\n",
    "    \n",
    "    # Get predictions\n",
    "    train_pred, train_prob = model.predict(X_train)\n",
    "    val_pred, val_prob = model.predict(X_val)\n",
    "    test_pred, test_prob = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics for each dataset\n",
    "    datasets = ['Training', 'Validation', 'Test']\n",
    "    y_true_list = [y_train, y_val, y_test]\n",
    "    y_pred_list = [train_pred, val_pred, test_pred]\n",
    "    y_prob_list = [train_prob, val_prob, test_prob]\n",
    "    \n",
    "    metrics_data = []\n",
    "    for dataset, y_true, y_pred, y_prob in zip(datasets, y_true_list, y_pred_list, y_prob_list):\n",
    "        metrics_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'Precision': precision_score(y_true, y_pred),\n",
    "            'Recall': recall_score(y_true, y_pred),\n",
    "            'F1-Score': f1_score(y_true, y_pred),\n",
    "            'AUC': roc_auc_score(y_true, y_prob),\n",
    "            'AP': average_precision_score(y_true, y_prob),\n",
    "            'MAE': np.mean(np.abs(y_prob - y_true))\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # 1. Bar chart comparison\n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC', 'AP']\n",
    "    x = np.arange(len(metrics_to_plot))\n",
    "    width = 0.25\n",
    "    \n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        values = [metrics_df[metrics_df['Dataset']==dataset][m].values[0] for m in metrics_to_plot]\n",
    "        offset = (idx - 1) * width\n",
    "        bars = ax1.bar(x + offset, values, width, label=dataset, alpha=0.8)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlabel('Metrics', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Performance Metrics Comparison Across Datasets', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(metrics_to_plot, rotation=0)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.set_ylim([0, 1.1])\n",
    "    \n",
    "    # 2. Table with detailed metrics\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    table_data = []\n",
    "    table_data.append(['Dataset', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC', 'AP', 'MAE'])\n",
    "    \n",
    "    for _, row in metrics_df.iterrows():\n",
    "        table_data.append([\n",
    "            row['Dataset'],\n",
    "            f\"{row['Accuracy']:.4f}\",\n",
    "            f\"{row['Precision']:.4f}\",\n",
    "            f\"{row['Recall']:.4f}\",\n",
    "            f\"{row['F1-Score']:.4f}\",\n",
    "            f\"{row['AUC']:.4f}\",\n",
    "            f\"{row['AP']:.4f}\",\n",
    "            f\"{row['MAE']:.4f}\"\n",
    "        ])\n",
    "    \n",
    "    table = ax2.table(cellText=table_data[1:], colLabels=table_data[0],\n",
    "                     cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    # Style the table\n",
    "    for i in range(len(table_data[0])):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    for i in range(1, len(table_data)):\n",
    "        for j in range(len(table_data[0])):\n",
    "            if i == 1:  # Training row\n",
    "                table[(i, j)].set_facecolor('#E3F2FD')\n",
    "            elif i == 2:  # Validation row\n",
    "                table[(i, j)].set_facecolor('#E8F5E9')\n",
    "            else:  # Test row\n",
    "                table[(i, j)].set_facecolor('#FFEBEE')\n",
    "    \n",
    "    ax2.set_title('Detailed Metrics Table', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.suptitle('Comprehensive Accuracy Validation Metrics', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/22_accuracy_metrics_comparison.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 22_accuracy_metrics_comparison.svg\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def plot_residual_analysis(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plot residual analysis to validate prediction quality\n",
    "    \"\"\"\n",
    "    output_dir = \"xgboost_plots\"\n",
    "    \n",
    "    # Get predictions\n",
    "    train_pred, train_prob = model.predict(X_train)\n",
    "    val_pred, val_prob = model.predict(X_val)\n",
    "    test_pred, test_prob = model.predict(X_test)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    train_residuals = y_train - train_prob\n",
    "    val_residuals = y_val - val_prob\n",
    "    test_residuals = y_test - test_prob\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Row 1: Residual plots\n",
    "    # Training residuals\n",
    "    axes[0, 0].scatter(train_prob, train_residuals, alpha=0.5, c='blue', s=30)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Predicted Probability', fontsize=18, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Residual (Actual - Predicted)', fontsize=18, fontweight='bold')\n",
    "    axes[0, 0].set_title(f'Training Set Residuals\\nMean={np.mean(train_residuals):.4f}, Std={np.std(train_residuals):.4f}', \n",
    "                         fontsize=18, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation residuals\n",
    "    axes[0, 1].scatter(val_prob, val_residuals, alpha=0.5, c='green', s=30)\n",
    "    axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Predicted Probability', fontsize=18, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Residual (Actual - Predicted)', fontsize=18, fontweight='bold')\n",
    "    axes[0, 1].set_title(f'Validation Set Residuals\\nMean={np.mean(val_residuals):.4f}, Std={np.std(val_residuals):.4f}', \n",
    "                         fontsize=18, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test residuals\n",
    "    axes[0, 2].scatter(test_prob, test_residuals, alpha=0.5, c='red', s=30)\n",
    "    axes[0, 2].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Predicted Probability', fontsize=18, fontweight='bold')\n",
    "    axes[0, 2].set_ylabel('Residual (Actual - Predicted)', fontsize=18, fontweight='bold')\n",
    "    axes[0, 2].set_title(f'Test Set Residuals\\nMean={np.mean(test_residuals):.4f}, Std={np.std(test_residuals):.4f}', \n",
    "                         fontsize=18, fontweight='bold')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 2: Q-Q plots (to check if residuals are normally distributed)\n",
    "    from scipy import stats\n",
    "    \n",
    "    for idx, (residuals, title, color) in enumerate([\n",
    "        (train_residuals, 'Training', 'blue'),\n",
    "        (val_residuals, 'Validation', 'green'),\n",
    "        (test_residuals, 'Test', 'red')\n",
    "    ]):\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=axes[1, idx])\n",
    "        axes[1, idx].get_lines()[0].set_color(color)\n",
    "        axes[1, idx].get_lines()[0].set_markersize(5)\n",
    "        axes[1, idx].get_lines()[1].set_color('red')\n",
    "        axes[1, idx].get_lines()[1].set_linewidth(2)\n",
    "        axes[1, idx].set_title(f'{title} Set Q-Q Plot', fontsize=18, fontweight='bold')\n",
    "        axes[1, idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Residual Analysis: Validation of Prediction Quality', fontsize=28, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/23_residual_analysis.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 23_residual_analysis.svg\")\n",
    "\n",
    "\n",
    "def plot_mae_learning_curve(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Plot MAE (Mean Absolute Error) Learning Curve\n",
    "    Shows how MAE changes as training sample size increases\n",
    "    \"\"\"\n",
    "    output_dir = \"xgboost_plots\"\n",
    "    \n",
    "    print(\"\\nGenerating MAE Learning Curve...\")\n",
    "    \n",
    "    # Define training sizes (from 10% to 100% of training data)\n",
    "    train_sizes = np.linspace(0.1, 1.0, 15)\n",
    "    \n",
    "    train_mae_scores = []\n",
    "    val_mae_scores = []\n",
    "    train_mae_std = []\n",
    "    val_mae_std = []\n",
    "    actual_train_sizes = []\n",
    "    \n",
    "    # For each training size, train multiple models and calculate MAE\n",
    "    for size in train_sizes:\n",
    "        n_samples = int(size * len(X_train))\n",
    "        if n_samples < 10:  # Skip if too few samples\n",
    "            continue\n",
    "        \n",
    "        actual_train_sizes.append(n_samples)\n",
    "        \n",
    "        # Multiple runs for stability (5 runs with different random samples)\n",
    "        temp_train_maes = []\n",
    "        temp_val_maes = []\n",
    "        \n",
    "        for run in range(5):\n",
    "            # Random subsample from training data\n",
    "            indices = np.random.choice(len(X_train), n_samples, replace=False)\n",
    "            X_sub = X_train[indices]\n",
    "            y_sub = y_train[indices]\n",
    "            \n",
    "            # Train a temporary model\n",
    "            temp_model = xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42 + run,\n",
    "                eval_metric='logloss',\n",
    "                use_label_encoder=False\n",
    "            )\n",
    "            temp_model.fit(X_sub, y_sub, verbose=False)\n",
    "            \n",
    "            # Calculate MAE for training and validation\n",
    "            train_pred_prob = temp_model.predict_proba(X_sub)[:, 1]\n",
    "            val_pred_prob = temp_model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            train_mae = np.mean(np.abs(y_sub - train_pred_prob))\n",
    "            val_mae = np.mean(np.abs(y_val - val_pred_prob))\n",
    "            \n",
    "            temp_train_maes.append(train_mae)\n",
    "            temp_val_maes.append(val_mae)\n",
    "        \n",
    "        # Calculate mean and std across runs\n",
    "        train_mae_scores.append(np.mean(temp_train_maes))\n",
    "        val_mae_scores.append(np.mean(temp_val_maes))\n",
    "        train_mae_std.append(np.std(temp_train_maes))\n",
    "        val_mae_std.append(np.std(temp_val_maes))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    train_mae_scores = np.array(train_mae_scores)\n",
    "    val_mae_scores = np.array(val_mae_scores)\n",
    "    train_mae_std = np.array(train_mae_std)\n",
    "    val_mae_std = np.array(val_mae_std)\n",
    "    actual_train_sizes = np.array(actual_train_sizes)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot training MAE with confidence interval\n",
    "    ax.plot(actual_train_sizes, train_mae_scores, 'o-', linewidth=3, \n",
    "            markersize=8, color='blue', label='Training MAE')\n",
    "    ax.fill_between(actual_train_sizes, \n",
    "                     train_mae_scores - train_mae_std,\n",
    "                     train_mae_scores + train_mae_std,\n",
    "                     alpha=0.2, color='blue')\n",
    "    \n",
    "    # Plot validation MAE with confidence interval\n",
    "    ax.plot(actual_train_sizes, val_mae_scores, 's-', linewidth=3,\n",
    "            markersize=8, color='red', label='Validation MAE')\n",
    "    ax.fill_between(actual_train_sizes,\n",
    "                     val_mae_scores - val_mae_std,\n",
    "                     val_mae_scores + val_mae_std,\n",
    "                     alpha=0.2, color='red')\n",
    "    \n",
    "    # Add horizontal line for final validation MAE\n",
    "    final_val_mae = val_mae_scores[-1]\n",
    "    ax.axhline(y=final_val_mae, color='green', linestyle='--', linewidth=2,\n",
    "               alpha=0.7, label=f'Final Validation MAE: {final_val_mae:.4f}')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel('Training Sample Size', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Mean Absolute Error (MAE)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('MAE Learning Curve: Model Performance vs Training Size', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "    \n",
    "    # Add text annotation for interpretation\n",
    "    gap = val_mae_scores[-1] - train_mae_scores[-1]\n",
    "    if gap > 0.05:\n",
    "        interpretation = \"High variance (overfitting)\"\n",
    "        color = 'red'\n",
    "    elif gap < 0.02:\n",
    "        interpretation = \"Good generalization\"\n",
    "        color = 'green'\n",
    "    else:\n",
    "        interpretation = \"Moderate generalization\"\n",
    "        color = 'orange'\n",
    "    \n",
    "    ax.text(0.02, 0.98, f'Final Training MAE: {train_mae_scores[-1]:.4f}\\n'\n",
    "                        f'Final Validation MAE: {val_mae_scores[-1]:.4f}\\n'\n",
    "                        f'Gap: {gap:.4f}\\n'\n",
    "                        f'Assessment: {interpretation}',\n",
    "            transform=ax.transAxes, fontsize=11, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor=color, alpha=0.2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/24_mae_learning_curve.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 24_mae_learning_curve.svg\")\n",
    "    \n",
    "    # Additional detailed plot with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left: MAE Learning Curve (same as above)\n",
    "    axes[0].plot(actual_train_sizes, train_mae_scores, 'o-', linewidth=3,\n",
    "                 markersize=8, color='blue', label='Training MAE')\n",
    "    axes[0].fill_between(actual_train_sizes,\n",
    "                          train_mae_scores - train_mae_std,\n",
    "                          train_mae_scores + train_mae_std,\n",
    "                          alpha=0.2, color='blue')\n",
    "    axes[0].plot(actual_train_sizes, val_mae_scores, 's-', linewidth=3,\n",
    "                 markersize=8, color='red', label='Validation MAE')\n",
    "    axes[0].fill_between(actual_train_sizes,\n",
    "                          val_mae_scores - val_mae_std,\n",
    "                          val_mae_scores + val_mae_std,\n",
    "                          alpha=0.2, color='red')\n",
    "    axes[0].set_xlabel('Training Sample Size', fontsize=18, fontweight='bold')\n",
    "    axes[0].set_ylabel('Mean Absolute Error (MAE)', fontsize=18, fontweight='bold')\n",
    "    axes[0].set_title('MAE Learning Curve', fontsize=18, fontweight='bold')\n",
    "    # 左图图例：添加边框和不透明白色背景\n",
    "    legend0 = axes[0].legend(fontsize=18, frameon=True, fancybox=True, shadow=True,\n",
    "                            framealpha=1.0, facecolor='white', edgecolor='black',\n",
    "                            loc='best')\n",
    "    legend0.get_frame().set_linewidth(1.5)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right: Gap between Training and Validation MAE\n",
    "    mae_gap = val_mae_scores - train_mae_scores\n",
    "    axes[1].plot(actual_train_sizes, mae_gap, 'o-', linewidth=3,\n",
    "                 markersize=8, color='purple', label='Validation - Training MAE')\n",
    "    axes[1].fill_between(actual_train_sizes, mae_gap, 0, alpha=0.3, color='purple')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    axes[1].axhline(y=0.05, color='red', linestyle='--', linewidth=2, alpha=0.5,\n",
    "                    label='High Variance Threshold (0.05)')\n",
    "    axes[1].set_xlabel('Training Sample Size', fontsize=18, fontweight='bold')\n",
    "    axes[1].set_ylabel('MAE Gap (Validation - Training)', fontsize=18, fontweight='bold')\n",
    "    axes[1].set_title('Overfitting Detection: MAE Gap Analysis', fontsize=18, fontweight='bold')\n",
    "     # 右图图例：添加边框和不透明白色背景\n",
    "    legend1 = axes[1].legend(fontsize=18, frameon=True, fancybox=True, shadow=True,\n",
    "                            framealpha=1.0, facecolor='white', edgecolor='black',\n",
    "                            loc='best')\n",
    "    legend1.get_frame().set_linewidth(1.5)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comprehensive MAE Learning Curve Analysis', fontsize=28, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/25_mae_learning_curve_detailed.svg', format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: 25_mae_learning_curve_detailed.svg\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== MAE Learning Curve Summary ===\")\n",
    "    print(f\"Initial Training MAE (10% data): {train_mae_scores[0]:.4f}\")\n",
    "    print(f\"Final Training MAE (100% data): {train_mae_scores[-1]:.4f}\")\n",
    "    print(f\"Initial Validation MAE: {val_mae_scores[0]:.4f}\")\n",
    "    print(f\"Final Validation MAE: {val_mae_scores[-1]:.4f}\")\n",
    "    print(f\"MAE Improvement (Validation): {val_mae_scores[0] - val_mae_scores[-1]:.4f}\")\n",
    "    print(f\"Final MAE Gap: {gap:.4f}\")\n",
    "    print(f\"Assessment: {interpretation}\")\n",
    "    \n",
    "    return {\n",
    "        'train_sizes': actual_train_sizes,\n",
    "        'train_mae': train_mae_scores,\n",
    "        'val_mae': val_mae_scores,\n",
    "        'train_std': train_mae_std,\n",
    "        'val_std': val_mae_std,\n",
    "        'final_gap': gap\n",
    "    }\n",
    "\n",
    "\n",
    "# [Keep all other existing functions unchanged]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with enhanced validation plots\"\"\"\n",
    "    print(\"=== XGBoost Time Series Binary Classification Detailed Analysis ===\\n\")\n",
    "    \n",
    "    # [All existing main() code remains the same until visualization section]\n",
    "    # ... (keep existing code)\n",
    "      # 1. Load data\n",
    "    print(\"1. Loading data...\")\n",
    "    X, y = load_excel_data('DATA-5.xlsx')\n",
    "    \n",
    "    if X is None or y is None:\n",
    "        print(\"Excel data loading failed, using simulated data...\")\n",
    "        X, y = generate_sample_data()\n",
    "    \n",
    "    print(f\"Data dimensions: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"Label distribution: Positive={np.sum(y)} ({100*np.mean(y):.1f}%), Negative={len(y)-np.sum(y)} ({100*(1-np.mean(y)):.1f}%)\")\n",
    "    \n",
    "    # 2. Data preprocessing\n",
    "    print(\"\\n2. Data preprocessing and splitting...\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data(X, y)\n",
    "    \n",
    "    # 3. Create and train XGBoost model\n",
    "    print(\"\\n3. Creating and training XGBoost model...\")\n",
    "    model = XGBoostDetailedModel()\n",
    "    \n",
    "    # Train model\n",
    "    val_acc, val_auc, val_f1 = model.train(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # 4. Cross validation\n",
    "    print(\"\\n4. Performing cross validation...\")\n",
    "    cv_scores = model.cross_validate(X_train, y_train, cv=5)\n",
    "    \n",
    "    # 5. Final test set evaluation\n",
    "    print(\"\\n5. Final test set evaluation...\")\n",
    "    test_pred, test_prob = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    test_auc = roc_auc_score(y_test, test_prob)\n",
    "    test_f1 = f1_score(y_test, test_pred)\n",
    "    \n",
    "    print(f\"\\n=== Final Test Results ===\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"Cross-validation AUC Mean: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, test_pred, \n",
    "                               target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    # 6. Comprehensive visualization analysis\n",
    "    print(\"\\n6. Generating detailed visualization analysis...\")\n",
    "    \n",
    "    # Main analysis charts\n",
    "    print(\"Plotting comprehensive analysis charts...\")\n",
    "    results = save_individual_plots(model, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    # Training process analysis\n",
    "    print(\"Plotting training process analysis...\")\n",
    "    plot_training_progress(model)\n",
    "    \n",
    "    # Feature analysis\n",
    "    print(\"Plotting feature interaction analysis...\")\n",
    "    if model.feature_importance is not None:\n",
    "        analyze_feature_interactions(model, X_train, model.feature_names)\n",
    "    else:\n",
    "        print(\"Skipping feature analysis (feature importance data unavailable)\")\n",
    "    \n",
    "    # Performance report table\n",
    "    print(\"Creating performance report table...\")\n",
    "    create_performance_report_table(y_test, test_pred, test_prob)\n",
    "    # After section 6, add new validation plots:\n",
    "    print(\"\\n7. Generating additional validation plots...\")\n",
    "    \n",
    "    print(\"Plotting actual vs predicted values...\")\n",
    "    plot_actual_vs_predicted(model, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    print(\"Creating accuracy metrics comparison...\")\n",
    "    metrics_comparison = plot_prediction_accuracy_metrics(model, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    print(\"\\nMetrics Comparison Summary:\")\n",
    "    print(metrics_comparison)\n",
    "    \n",
    "    print(\"Performing residual analysis...\")\n",
    "    plot_residual_analysis(model, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    print(\"Performing mae_learning_curve...\")\n",
    "    plot_mae_learning_curve(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # [Rest of existing code continues...]\n",
    "    # 7. Model interpretation and recommendations\n",
    "    print(\"\\n=== XGBoost Model Analysis Summary ===\")\n",
    "    print(f\"✓ Model achieved AUC of {test_auc:.3f} on test set, performance is {'Excellent' if test_auc > 0.8 else 'Good' if test_auc > 0.7 else 'Fair'}\")\n",
    "    print(f\"✓ Cross-validation results are stable, AUC standard deviation is {cv_scores.std():.3f}\")\n",
    "    \n",
    "    if model.feature_importance is not None:\n",
    "        most_important = model.feature_names[np.argmax(model.feature_importance)]\n",
    "        print(f\"✓ Most important feature is {most_important}, importance: {np.max(model.feature_importance):.3f}\")\n",
    "        \n",
    "        # Feature importance explanation\n",
    "        importance_ranking = sorted(zip(model.feature_names, model.feature_importance), \n",
    "                                  key=lambda x: x[1], reverse=True)\n",
    "        print(\"\\nFeature Importance Ranking:\")\n",
    "        for i, (feature, importance) in enumerate(importance_ranking, 1):\n",
    "            print(f\"{i}. {feature}: {importance:.3f}\")\n",
    "    else:\n",
    "        print(\"✗ Unable to obtain feature importance information\")\n",
    "    \n",
    "    print(f\"\\nRecommend using this XGBoost model for time series binary classification prediction\")\n",
    "    \n",
    "    # 8. Validate Real-Data.xlsx\n",
    "    print(\"\\n8. Validating Real-Data.xlsx...\")\n",
    "    real_data_results = validate_real_data(model, scaler)\n",
    "    \n",
    "    print(\"\\n=== Real-Data Validation Complete ===\")\n",
    "    print(f\"All plots have been saved in 'real_data_plots' directory\")\n",
    "    return model, results, real_data_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, results, real_data_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b7280a-75a9-493e-998f-38658c9a4c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numpy': '1.23.5',\n",
       " 'pandas': '2.2.1',\n",
       " 'matplotlib': '3.10.0',\n",
       " 'scikit-learn': '1.6.1',\n",
       " 'xgboost': '3.0.1',\n",
       " 'seaborn': '0.13.2'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, matplotlib, sklearn\n",
    "import seaborn as sns\n",
    "import xgboost\n",
    "\n",
    "vers = {\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"matplotlib\": matplotlib.__version__,\n",
    "    \"scikit-learn\": sklearn.__version__,\n",
    "    \"xgboost\": xgboost.__version__,\n",
    "    \"seaborn\": sns.__version__,\n",
    "}\n",
    "\n",
    "vers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176eedad-613d-4bd6-9afe-a054a384d6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
